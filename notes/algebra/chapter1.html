<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Linear Algebra</title>
  <link rel="stylesheet" href="../../css/style.css" />
  <link rel="stylesheet" href="../../css/katex.min.css" />
  <script defer src="../../js/katex.min.js"></script>
  <script defer src="../../js/auto-render.min.js"
    onload="renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '\\[', right: '\\]', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\(', right: '\\)', display: false}
      ]
    });">
  </script>
</head>
<body>
  <main>
    <header>
      <h1>
        <a href="introduction.html" aria-label="Back to Introduction">&lt;</a>
        Chapter 1: Linear Equations
      </h1>
    </header>

    <section id="Topic1">
      <h2>1.1. Systems of Linear Equations</h2>

      <h3>Linear Equations and Systems</h3>
      <p>A <b>linear equation</b> in the variables \(x_1, x_2, ..., x_n\) is an
      equation that can be written in the form
      \[a_1x_1 + a_2x_2 + \dotsc + a_nx_n = b\]
      where \(b\) and the <b>coefﬁcients</b> \(a_1, a_2, \dotsc, a_n\) are
      real or complex numbers, usually known in advance.
      </p>

      <p>Example: \(\ 4x_1 - 5x_2 = 2\)</p>

      <p>A <b>system of linear equations</b> (or a <b>linear system</b>)
      is a collection of one or more linear equations involving
      the same variables, \(x_1, x_2, \dotsc, x_n\) .
      In general, a system of linear equations looks like this:
      \[a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n = b_1\]
      \[a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n = b_2\]
      \[ \quad\quad\quad\quad\quad\vdots\quad\quad\quad\quad\quad\ = \vdots\]
      \[a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n = b_m\]
      We say that it is a system with \(m\) <b>equations</b> in \(n\) <b>unknowns</b>.
      </p>

      <p>Example: \[\ 2x_1 + x_2 = 3\]
      \[x_1 - x_2 = 0\]
      </p>

      <h3>Solutions of Linear Systems</h3>
      <p>A <b>solution</b> of a linear system is an ordered list of numbers
      \((s_1, s_2, \dotsc, s_n)\) which satisfies each equation in the system.
      That is, if the values \(s_1, s_2, \dotsc, s_n\) are substituted
      in for \(x_1, x_2, \dotsc, x_n\), then each equation is true.
      </p>

      <p>The set of all possible solutions is called the <b>solution set</b>
      of the linear system.Two linear systems are called <b>equivalent</b>
      if they have the same solution set.</p>

      A system of linear equations has either
      <ul>
        <li>a unique solution</li>
        <li>infinitely many solutions</li>
        <li>no solutions</li>
      </ul>

      <p>A system of linear equations is said to be <b>consistent</b>
      if it has either one solution or inﬁnitely many solutions.
      A system is <b>inconsistent</b> if it has no solution.</p>

      <h3>Matrix Notation</h3>
      <p>Given a linear system,\[a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n = b_1\]
      \[a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n = b_2\]
      \[ \quad\quad\quad\quad\quad\vdots\quad\quad\quad\quad\quad\ = \vdots\]
      \[a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n = b_m\]
 
      we write the system in <b>augmented matrix</b> form
      \[\begin{vmatrix}a_{11} & a_{12} & \dots & a_{1n} & b_1\\
      a_{21} & a_{22} & \dots & a_{2n} & b_2\\
      \vdots & \vdots & \ddots & \vdots & \vdots\\
      a_{m1} & a_{22} & \dots & a_{mn} & b_m\end{vmatrix}\]
      An \(m\) x \(n\) matrix is a rectangular array of numbers with
      \(m\) rows and \(n\) columns.
      </p>

      <h3>Elementary Row Operations</h3>
      <p>Once we have the system in augmented matrix form,
      we apply so-called <b>elementary row operations</b>. Doing so has the
      effect of replacing the original linear system with a simpler one,
      whose solutions are much easier to calculate.
      </p>
      There are three types of elementary row operations
      <ol>
        <li><b>Replacement</b>: Replace one row with the sum of itself and a
          scalar multiple of another row.
        </li>
        <li><b>Interchange</b>: Interchange two rows.</li>
        <li><b>Scaling</b>: Multiply all entries in a row by a non-zero scalar.</li>
      </ol>

      <p>Two matrices are called <b>row-equivalent</b> if one can be transformed
      into the other by applying a finite number of elementary row operations.
      </p>
    </section>

    <section id="Topic2">
      <h2>1.2. Row Reduction and Echelon Forms</h2>

      <h3>Row Echelon Form</h3>
      <p>A <b>zero row</b> in a matrix is a row where each entry in the row is zero.
      A <b>non-zero row</b> in a matrix is a row which contains at least one
      non-zero entry. A <b>leading entry</b> in a non-zero row is the first
      non-zero entry in that row (reading from left to right).
      </p>

      We say that a matrix is in <b>row echelon form</b> (or <b>echelon form</b>)
      if it satisfies the following conditions:
      <ul>
        <li>All the non-zero rows are above the zero rows.</li>
        <li>Every leading entry in a row is in a column to the right of the
          leading entry in the row above it.
        </li>
        <li>All entries in a column below a leading entry are zeros.</li>
      </ul>

      We say that a matrix is in <b>reduced row echelon form</b>
      (or <b>reduced echelon form</b>) if it is firstly in row echelon form and
      then further satisfies the following conditions:
      <ul>
        <li>Each leading entry in a non-zero row is 1 (called a <b>leading 1</b>).</li>
        <li>Each leading 1 is the only non-zero entry in its column.</li>
      </ul>

      <p><b>Theorem 1</b><br>Every matrix is row-equivalent to a unique
      (i.e. one and only one) matrix which is in reduced row echelon form.
      </p>

      <h3>Solutions of a Linear System</h3>
      <p>A <b>pivot position</b> of a matrix \(A\) is a position in \(A\) which
      corresponds to the position of a leading 1 in the reduced row echelon
      form of \(A\). A <b>pivot column</b> of \(A\) is a column of \(A\) which
      contains a pivot position.
      </p>

      <p>The variables in a linear system which correspond to pivot
      columns in the augmented matrix of the linear system are called
      the <b>leading variables</b> (or the <b>basic variables</b>).
      The remaining variables are called <b>free variables</b>.
      </p>

      <p><b>Theorem 2</b><br>
      A linear system is consistent (i.e. has a solution) if and ony if the right-most
      column of the augmented matrix of the system is <b>NOT</b> a pivot column,
      that is, if and only if the augmented matrix has no row of the form
      \[\begin{bmatrix}0 & \dots & 0 & b\end{bmatrix}\] with with b nonzero.
      If a linear system is consistent, then the solution set contains either
      <ul>
        <li>a unique solution, if there are no free variables.</li>
        <li>infinitely many solutions, if there is at least one free variable.</li>
      </ul>
      </p>
   </section>

    <section id="Topic3">
      <h2>1.3. Vector Equations</h2>
      <h3>Column Vectors</h3>
      <p>A matrix with only one column is called a <b>column vector</b>,
      or simply a <b>vector</b>.
      \[\vec{x} = \begin{vmatrix}x_1 \\ \vdots \\ x_n\end{vmatrix}\]
      The set of all n × 1 vectors (with real entries) is called \(\real^n\) .
      That is, <br>\(\real^n = \{ \begin{vmatrix}x_1 \\ \vdots \\x_n\end{vmatrix}
      \ | \ x_i \in \real\) for each \(1 \le i \le n \ \}\)
      </p>

      <p>The <b>zero vector</b> denoted by \(\vec{0}\), is the vector in
      \(\real^n\) with \(0\) in each entry.</p>

      <h3>Vector Operations in \(\real^n\)</h3>
      <h4>Equality</h4>
      <p>We say that two vectors \(\vec{u}\) and \(\vec{v}\) in \(\real^n\)
      are <b>equal</b>, and we write \(\vec{u} = \vec{v}\),
      if all corresponding entries are equal. That is, \(u_i = v_i\) for
      each \(1 \le i \le n\) .
      </p>

      <h4>Sum</h4>
      <p>If \(\vec{u}\) and \(\vec{v}\) are two vectors in \(\real^n\)
      then we can compute the <b>sum</b> \(\vec{u}\ + \vec{v}\) as the
      sum of the corresponding entries of \(\vec{u}\) and \(\vec{v}\) .
      That is, \((\vec{u} + \vec{v})_i = \vec{u}_i + \vec{v}_i\)
      </p>

      <h4>Scalar multiplication</h4>
      <p>If \(v\) is in \(\real^n\) and \(k \in \real\) is a scalar,
      then the <b>scalar product</b> \(k\vec{v}\) is a vector in \(\real^n\)
      and \((k\vec{v})_i = k\vec{v}_i\) for each \(1 \le i \le n\) .
      That is, we simply multiply each entry of \(\vec{v}\) with \(k\) .
      </p>

      <h3>Algebraic properties of \(\real^n\)</h3>
      For all \(\vec{u}, \vec{v}, \vec{w}\), and scalars \(c\) and \(d\), we have
      <ol>
        <li>\(\ \ \vec{u} + \vec{v} = \vec{v} + \vec{u}\)</li>
        <li>\(\ \ (\vec{u}+\vec{v})+\vec{w}=\vec{u}+(\vec{v}+\vec{w})\)</li>
        <li>\(\ \ \vec{u}+\vec{0}=\vec{u}\)</li>
        <li>\(\ \ \vec{u}+(-\vec{u})=\vec{0}\), where by \(-\vec{u}\)
          we mean \(-1\vec{u}\)
        </li>
        <li>\(\ \ c\ (\vec{u}+\vec{v})=c\vec{u}+c\vec{v}\)</li>
        <li>\(\ \ (c+d)\vec{u}=c\vec{u}+d\vec{u}\)</li>
        <li>\(\ \ c\ (d\vec{u}\ )=(cd\ )\vec{u}\)</li>
        <li>\(\ \ 1\vec{u}=\vec{u}\)</li>
      </ol>

      <h3>Linear combinations</h3>
      <p>Let \(\vec{v_1}, \vec{v_2}, \dots, \vec{v_p}\) be in \(\real^n\)
      and let \(c_1, c_2, \dots, c_p\) be scalars. The expression
      \[c_1\vec{v_1} + c_2\vec{v_2} + \dots + c_p\vec{v_p}\]
      is called a <b>linear combination</b> of the vectors
      \(\vec{v_1}, \vec{v_2}, \dots, \vec{v_n}\) with <b>weights</b>
      \(c_1, c_2, \dots, c_p\) .
      </p>

      <p><b>Terminology</b>: We say that \(\vec{y}\) is a linear combination of
      \(\vec{v_1}, \vec{v_2}, \dots, \vec{v_p}\) if there exist weights
      \(c_1, c_2, \dots, c_p\) such that
      \[\vec{y} = c_1\vec{v_1}+c_2\vec{v_2}+\dots+c_p\vec{v_p}\]
      </p>

      <h3>Solutions of vector equations</h3>
      <p>The vector equation
      \[x_1\vec{a_1}+x_2\vec{a_2}+\dots+x_n\vec{a_n} = \vec{b}\]
      has the same solution set as the linear system whose augmented matrix is
      \[\begin{vmatrix}\vec{a_1}&\vec{a_2}&\dots&\vec{a_n}&\vec{b}\end{vmatrix}\]
      In other words, \(\vec{b}\) is a linear combination of
      \(\vec{a_1}, \vec{a_2}, \dots, \vec{a_n}\) if and only if the linear
      system associated with the matrix is consistent.
      </p>

      <h3>The span of a set of vectors</h3>
      <p>If \(\vec{v_1}, \vec{v_2}, \dots, \vec{v_p}\) are in \(\real^n\)
      then Span{\(\vec{v_1}, \vec{v_2}, \dots, \vec{v_p}\)} is the set of
      all linear combinations of \(\vec{v_1}, \vec{v_2}, \dots,\vec{v_n}\) .
      That is,<br> Span\(\{\vec{v_1}, \vec{v_2}, \dots, \vec{v_p}\} =
      \{ c_1\vec{v_1} + c_2\vec{v_2} + \dots + c_p\vec{v_p}\ |\ c_i\ \in
      \real\) for each \(1 \le i \le p\}\) .
      </p>

      Let \(\vec{v}\) be a nonzero vector in \(\real^3\). Then Span{\(\vec{v}\)}
      is a line in \(\real^3\) through the origin.
      <div class="container">
        <img src="../../images/algebra/f1.3a.png" alt="figure 1.3a" class="figures"/>
      </div>

      If \(\vec{u}\) and \(\vec{v}\) are both nonzero vectors such that
      \(\vec{u}\) is not a scalar multiple of \(\vec{v}\), then Span{\(\vec{u},
      \vec{v}\ \)} is a plane in \(\real^3\) through the origin.
      <div class="container">
        <img src="../../images/algebra/f1.3b.png" alt="figure 1.3b" class="figures"/>
      </div>
 

    </section>

    <section id="Topic4">
      <h2>1.4. The Matrix Equation \(A\vec{x} = \vec{b}\)</h2>
      <h3>The Product of a Matrix and a Vector</h3>
      <p>If \(A\) is and \(m\) x \(n\) matrix with columns
      \(\vec{a_1}, \vec{a_2}, \dots, \vec{a_n}\) and if
      \(\vec{x_n}\) is in \(\real^n\), then the <b>product</b> of
      \(A\) and \(\vec{x}\), denoted by \(A\vec{x}\) ,
      is the linear combination of the columns of \(A\) using the entries of
      \(\vec{x}\) as the weights. That is,
      \[A\vec{x}\ =\ \begin{vmatrix}\vec{a_1}&\vec{a_2}&\dots&\vec{a_n}\end{vmatrix}
      \begin{vmatrix}x_1\\x_2\\ \vdots\\ x_n\end{vmatrix} \ =\ x_1\vec{a_1} +
      x_2\vec{a_2} + \dots + x_n\vec{a_n}\]
      </p>

      <h4>Theorem 3</h4>
      <p>If \(A\) is an \(m\) x \(n\) matrix with columns
      \(a_1, a_2, \dots, a_n\) and if \(\vec{b}\) is in \(\real^m\),
      then the matrix equation \[A\vec{x} = \vec{b}\]
      has the same solutions as the vector equation
      \[x_1\vec{a_1}+x_2\vec{a_2}+\dots+x_n\vec{b_n}=\vec{b}\]
      which in turn has the same solutions as the linear system which
      corresponds to the augmented matrix
      \[\begin{vmatrix}a_1 & a_2 & \dots & a_n & \vec{b}\end{vmatrix}\]
      We call \(A\) the <b>coefficient matrix</b> of the linear system.
      </p>

      <h3>Properties of the Coefficient Matrix</h3>
      <ul>
        <li>For each \(\vec{b}\in\real^m\), the equation
          \(A\vec{x}=\vec{b}\) has a solution.
        </li>
        <li>Each \(\vec{b}\in\real^m\) is a linear combination
          of the columns of \(A\) .
        </li>
        <li>The columns of \(A\) span \(\real^m\).</li>
        <li>\(A\) has a pivot position in each row.</li>
      </ul>

      <h3>The Row-Column Rule for Calculating \(A\vec{x} = \vec{b}\)</h3>
      <p>Let \(A\) be an \(m\) x \(n\) matrix and let \(\vec{x}\) be an
      \(n\) x \(1\) column vector. Then the \(i\)-th entry in \(A\vec{x}\)
      is the sum of the products of the corresponding entries in row \(i\)
      of \(A\) and the entries in \(\vec{x}\) . That is,
      \[A\vec{x} = \begin{vmatrix}a_{11}& a_{12}& \dots& a_{1n}\\
      a_{21}& a_{22}& \dots& a_{2n}\\ \vdots& \vdots& \ddots& \vdots\\
      a_{m1}& a_{m2}& \dots& a_{mn}\end{vmatrix}
      \begin{vmatrix}x_1\\x_2\\ \vdots\\ x_n\end{vmatrix}
      = \begin{vmatrix}a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n\\
      a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n\\
      \vdots\\a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n\end{vmatrix}\]
      In other words, the \(i\)-th entry of \(A\vec{x}\)
      is obtained by taking the dot product of the \(i\)-th "row vector"
      of \(A\) and \(\vec{x}\) .
      </p>

      <h3>Properties of the Product \(A\vec{x}\)</h3>
      If \(A\) is an \(m\) x \(n\) matrix, and if
      \(\vec{u}\) and \(vec{v}\) are vectors in \(\real^n\),
      and \(c\) is a scalar, then:
      <ol>
        <li>\(\ \ \ A\ (\vec{u} + \vec{v}) = A\vec{u}+A\vec{v}\)</li>
        <li>\(\ \ \ A\ (c\vec{u}) = c\ (A\vec{u})\)</li>
      </ol>

    </section>

    <section id="Topic5">
      <h2>1.5. Solution Sets of Linear Systems</h2>
      <h3>Homogeneous Linear Systems</h3>
      <p>A linear system is called <b>homogeneous</b> if it can be written
      in the form \[A\vec{x} = \vec{0}\]
      Note that a homogeneous system always has at least one solution,
      the <b>trivial solution</b> \(\vec{x}\) = \(\vec{0}\) , because
      \(A\vec{0} = \vec{0}\).
      In other words, it is impossible for a homogeneous linear system
      to be inconsistent.
      </p>

      <p>Homogeneous systems will often have other solutions in addition
      to the trivial solution. These are called <b>non-trivial solutions</b>.
      A homogeneous linear system has non-trivial solutions if and only
      if it has at least one free variable.
      </p>

      <h3>Homogeneous vs Non-homogeneous Solutions</h3>
      <h4>Theorem 4</h4>
      <p>Suppose there is a vector \(\vec{b}\) such that the equation
      \(A\vec{x} = \vec{b}\) is consistent, and that \(\vec{p}\)
      is one of the solutions. Then the solution set of \(A\vec{x} = \vec{b}\)
      is the set of all vectors of the form
      \[\vec{w} = \vec{p} + \vec{v_h}\]
      where \(\vec{v_h}\) is any solution of the homogeneous equation
      \(A\vec{x} = \vec{0}\) .
      </p>

      <p>That is, the solution set of \(A\vec{x} = \vec{b}\) and
      \(A\vec{x} = \vec{0}\) are parallel as subsets of \(\real^n\)
      (if \(A\vec{x} = \vec{b}\) is consistent in the first place).
      We simply shift the solutions of \(A\vec{x} = \vec{0}\)
      along the vector \(\vec{p}\) to find the solutions of
      \(A\vec{x} = \vec{b}\) .
      </p>
      <div class="container">
        <img src="../../images/algebra/f1.5a.png" alt="figure 1.3a" class="figures"/>
      </div>

    </section>

    <section id="Topic6">
      <h2>1.6. Linear Independence</h2>
      <p>An indexed set of vectors {\(\vec{v_1}, \vec{v_2}, \dots, \vec{v_p}\)}
      is called <b>linearly independent</b> if
      \[x_1\vec{v_1} + x_2\vec{v_2} + \dots + x_p\vec{v_p} = \vec{0}\]
      has only the trivial solution \(\vec{x} = 0\) .
      </p>

      <p>The set {\(\vec{v_1}, \vec{v_2}, \dots, \vec{v_p}\)}
      is called <b>linearly dependent</b> if there exist weights
      \(c_1, c_2, \dots, c_p\) , not all zero, such that
      \[c_1\vec{v_1} + c_2\vec{v_2} + \dots + c_p\vec{v_p} = \vec{0}\]
      </p>

      <p>In the dependent case, the equation
      \(c_1\vec{v_1} + c_2\vec{v_2} + \dots + c_p\vec{v_p} = \vec{0}\)
      is called a <b>linear dependence relation</b> between
      \(\vec{v_1}, \vec{v_2}, \dots, \vec{v_p}\) .
      </p>

      <p>The columns of a matrix \(A\) are linearly independent if and only if
      \(A\vec{x} = \vec{0}\) has only the trivial solution.
      </p>

      <h3>The Geometry of Linear Independence</h3>
      <p>A set of exactly two vectors is linearly depedent if and only if
      one is a scalar multiple of the other.
      </p>
      <div class="container">
        <img src="../../images/algebra/f1.6a.png" alt="figure 1.6a" class="figures"/>
      </div>
       <div class="container">
        <img src="../../images/algebra/f1.6b.png" alt="figure 1.6b" class="figures"/>
      </div>
 
      <h4>Theorem 5</h4>
      <p>An indexed set \(S = \{\vec{v_1}, \vec{v_2}, \dots, \vec{v_n}\}\)
      of two or more vectors is linearly dependent if and only if at least
      of the vectors is a linear combination of the remaining vectors.
      </p>
      
      <p>Moreover, if \(S\) is linearly dependent and \(\vec{v_1}\ {=}\llap{/\,}\ \vec{0}\),
      then some \(\vec{v_j}\) is a linear combination of the preceding vectors
      \(\vec{v_1}, \vec{v_2}, \dots, \vec{v_{j-1}}\) then some \(\vec{v_j}\)
      is a linear combination of the preceding vectors
      \(\vec{v_1}, \vec{v_2}, \dots, \vec{v_{j-1}}\ \).
      </p>
      <div class="container">
        <img src="../../images/algebra/f1.6c.png" alt="figure 1.6c" class="figures"/>
      </div>
       <div class="container">
        <img src="../../images/algebra/f1.6d.png" alt="figure 1.6d" class="figures"/>
      </div>
 
      <h4>Theorem 6</h4>
      <p>If a set contains more vectors than there are entries in each vector,
      then the set is linearly dependent. That is, any set {
      \(\vec{v_1}, \vec{v_2}, \dots, \vec{v_p}\)} of vectors in \(\real^n\)
      is linearly dependent if \(p > n \) .
      </p>
      <div class="container">
        <img src="../../images/algebra/f1.6e.png" alt="figure 1.6e" class="figures"/>
        A linearly dependent set in \(\real^2\)
      </div>
 
      <h4>Theorem 7</h4>
      <p>Any set which contains the zero vector is linearly dependent.</p>
    </section>

    <section id="Topic7">
      <h2>1.7. Introduction to Linear Transformations</h2>
      <p>A <b>transformation</b> (or <b>function</b> or <b>mapping</b> or <b>map</b>)
      \(T : \real^n \rightarrow \real^m\) is a rule which assigns to each vector
      \(\vec{v}\) in \(\real^n\) a unique vector \(T(\vec{v}\ )\) in \(\real^m\).
      </p>

      <p>The set \(\real^n\) is called the <b>domain</b> of \(T\) and the set
      \(\real^m\) is called the <b>codomain</b> of \(T\). For \(\vec{x}\) in
      \(\real^n\), we call \(T(\vec{x}\ )\) the <b>image</b> of \(\vec{x}\)
      (under the action of \(T\) ). The set of all images
      \(T(\vec{x})\) is called the <b>range</b> of \(T\).
      </p>
      <div class="container">
        <img src="../../images/algebra/f1.7a.png" alt="figure 1.7a" class="figures"/>
      </div>

      <h3>Matrix Transformations</h3>
      <p>A <b>matrix transformation</b> is a transformation
      \(T : \real^n \rightarrow \real^m\) of the form
      \[T(\vec{x}\ ) = A\vec{x}\]
      where \(A\) is an \(m\) x \(n\) matrix.
      </p>
       <div class="container">
        <img src="../../images/algebra/f1.7b.png" alt="figure 1.7b" class="figures"/>
      </div>

      <h3>Linear Transformations</h3>
      A transformation \(T : \real^n \rightarrow \real^m\) is called
      <b>linear</b> if it satisfies the following two properties for all vectors
      \(\vec{u}, \vec{v} \in \real^n\) and all scalars \(c\).
      <ol>
        <li>\(\ T(\vec{u} + \vec{v}\ ) = T(\vec{u}\ ) + T(\vec{v}\ )\)
        </li>
        <li>\(\ T(c\vec{u}\ ) = cT(\vec{u}\ )\)</li>
      </ol>

      <h3>Properties of Linear Transformations</h3>
      Let \(T : \real^n \rightarrow \real^m\) be a linear map. Then
      <ol>
        <li>\(\ T(\ \vec{0}\ ) = \vec{0}\)</li>
        <li>\(\ T(r_1\vec{v_1}+\dots+r_p\vec{v_p})=r_1T(\vec{v_1})+\dots+r_pT(\vec{v_p})\)
          for all \(\vec{v_i} \in \real^n\) and \(r_j \in \real\) .
        </li>
      </ol>

    </section>

    <section id="Topic8">
      <h2>1.8. The Matrix of a Linear Transformation</h2>
    </section>

    <nav>
      <p>
        [<a href="introduction.html">Prev</a>]
        [<a href="introduction.html">Contents</a>]
        [<a href="../../index.html">Home</a>]
        [<a href="chapter2.html">Next</a>]
      </p>
    </nav>
  </main>
</body>
</html>
